daemon off;
#Heroku dynos have at least 4 cores.
worker_processes <%= ENV['NGINX_WORKERS'] || 4 %>;

events {
	use epoll;
	accept_mutex on;
	worker_connections 1024;
}

http {
	set_real_ip_from 10.0.0.0/8;
	set_real_ip_from 127.0.0.0/24;
	real_ip_header X-Forwarded-For;
	real_ip_recursive on;

	gzip on;
	gzip_comp_level 2;
	gzip_proxied any;
	gzip_min_length 512;
	gzip_types text/plain text/css application/json application/javascript application/x-javascript text/javascript text/xml application/xml application/rss+xml application/atom+xml application/rdf+xml image/svg+xml;

	server_tokens off;

	log_format l2met 'measure#nginx.service=$request_time request_id=$http_x_request_id';
	access_log logs/nginx/access.log l2met;
	error_log logs/nginx/error.log;

	include mime.types;
	default_type application/octet-stream;
	sendfile on;

	client_body_timeout 30;
	client_max_body_size 50m;

	upstream app_server {
		server localhost:3001 fail_timeout=0 max_fails=3;
 	}

	map $http_user_agent $isbot_ua {
		default 0;
		~*(BLEXBot|GoogleBot|bingbot|YandexBot|mj12bot|PaperLiBot|DotBot|AppleBot|MBCrawler|Buck\b|Java\/|curl|wget|Nutch|CCBot|AppEngine|moreover|python-requests) 1;
	}
	map $isbot_ua $limit_bot {
		0 "";
		1 $binary_remote_addr;
	}

	limit_req_zone $binary_remote_addr zone=app:10m rate=50r/m;
	limit_req_zone $limit_bot zone=bots:10m rate=40r/m;

	server {
		listen <%= ENV["PORT"] %>;
		server_name _;
		keepalive_timeout 5;

		set $botck start;
		set $accck no;
		# These bots are too fast, badly behaved, or clearly not valuable, so we block them.
		if ($request_uri = /robots.txt) {
			set $botck "{botck}txt";
		}
		if ($http_user_agent ~* (ahrefsbot|semrushbot|mauibot|moreover|megaindex)) {
			set $botck "${botck}bad";
		}
		# Unconditionally block UAs that are sending no UA string, or just a '-'.
		if ($http_user_agent ~* "^-?$") {
			set $botck "${botck}bad";
		}
		# Block UAs that look like browsers or other known entities, but have no Accept header.
		if ($http_user_agent ~ Gecko) {
			set $accck "start";
		}
		if ($http_user_agent ~ Googlebot) {
			set $accck "start";
		}
		if ($http_user_agent ~ bingbot) {
			set $accck "start";
		}
		if ($http_user_agent ~ feedly) {
			set $accck "whitelist";
		}
		if ($http_accept ~ "^-?$") {
			set $accck "${accck}bad";
		}
		# If either of these rules match, serve a 404. 404 is chosen because it's most likely to be recognized.
		if ($botck = "startbad") {
			return 404;
		}
		if ($accck = "startbad") {
			return 404;
		}

		# Unconditionally block this one, super-persistent, broken, and overeager crawler.
		if ($http_user_agent ~* "link Gecko") {
			return 404;
		}
		if ($http_user_agent ~* "KHTML,like") {
			return 404;
		}
		if ($http_user_agent ~* "Mozilla/5.0\(") {
			return 404;
		}
		# Proper browsers use "https://google.com/" with a trailing slash as a referer.
		# They also don't send the empty string as their referer.
		valid_referers none server_names ~.+;
		if ($invalid_referer) {
			return 404;
		}
		if ($http_referer = "https://google.com") {
			return 404;
		}
		if ($http_referer = "https://www.google.com") {
			return 404;
		}

		location ~ ^/assets/ {
			add_header X-Content-Type-Options nosniff;
			add_header Cache-Control public;
			root ./;
			expires max;
		}

		add_header Strict-Transport-Security "max-age=31536000" always;
		add_header Vary 'Accept, Accept-Encoding, Cookie';
		proxy_set_header Host $http_host;
		proxy_set_header X-Real-Ip $remote_addr;
		proxy_redirect off;
		if ($http_x_forwarded_proto != 'https') {
			rewrite ^ https://$host$request_uri? permanent;
		}

		location / {
			proxy_pass http://app_server;
			limit_req zone=app burst=6 nodelay;
			limit_req zone=bots burst=3;
			limit_req_status 429;
		}

		location /ip {
			default_type text/plain;
			return 200 $remote_addr;
		}
	}
}
